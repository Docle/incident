---
title: 记仇小本本
---

## Request 403 Forbidden

date: 2018-11-6

## BeautifulSoup 对象的选择域

* date: 2018-11-9

* brief：

  如下先在某文件中找到一个 `div` 标签，接着想找到该标签下的直接子 `li` 标签：

  ```python
  a = """
  <div>
  <li></li>
  <li><li></li></li>
  <li></li>
  </div>
  """
  soup = BeautifulSoup(a, 'lxml')
  div = soup.find('div')
  li = div.select('div > li')
  print(li)
  ```

  然而执行代码会发现没有找到任何标签。

* solution

  对 select 和 find 来说，选择的标签应该是在在其内部的标签，对于上面选择到的 `div` ，打印出来如下：

  ```

  ```

  虽然看到最外层是一个 `div` 标签，但对 `div` 这个对象来说其内部并没有 `div` 标签，或者把 `soup` 通过 `find('div')` 找到的 `div` 理解为最外部的 `div` 标签是标识所找到的对象的，而其实际包含的内容如下更好理解：

  ```

  ```

  因此 `div.select('div > li')` 会找不到 `div` 标签，也就找不到 `li` 标签了。如下便可以找到想要的标签：

  ```
  soup.select('div > li')
  ```

  ​

##BeautifulSoup select 的优雅写法

date: 2018-11-14

brief：

刚开始使用 BeautifulSoup 的时候习惯的使用 find 方法，后来发现 select 方法与css 选择器一样写起来更简洁开始转用 select 方法。但不自觉的就受到 find 方法的影响，比如一下的一段某个html 文本的中的一段，用 find 方法和 select 方法通过 class 属性找到该 div 标签，然后找到其下的 ul 标签的：

```python
from bs4 import BeautifulSoup

a = """
<div class='swf-contA'>
<h2 class='fs16'>步步高教育电子招聘概况</h2>
<ul class='fs16'>
<li>
        <dfn>在招职位：</dfn>
    </li>
    <li>
        <dfn>人员结构：</dfn>
    </li>
    <li>
        <dfn>历年招聘：</dfn>
    </li>
</ul>
</div>
"""
soup = BeautifulSoup(a, 'lxml')
# find 方法
ul = soup.find('div', attrs={'class': 'swf-contA'}).find('ul')
# select 方法
ul = soup.select_one('div.swf-contA').select_one('ul')
```

看起来 select 已经比 find 要简洁了不少，但其实还有更优雅的写法：

```python
ul = soup.select_one('div.swf-contA ul')
```

这样写还有一个好处是，在爬虫中因为有很多变数，所以会有很多判断，如判断是否真的存在找到 ul，如果是连续的 select 或 find 方法如下：

```python
ul = soup.select_one('div.swf-contA').select_one('ul')
if ul:
    print('success')
else:
    print('fail')
```

正常来说以上代码没有问题，但如果爬取的页面改版了程序就不在像预期的运行了，例如以下：

```python
ul = soup.select_one('div.swf').select_one('ul')
if ul:
    print('success')
else:
    print('fail')
```

执行代码不会打印 `fail` ，而是抛出了错误：

```

```

在第一个 select_one 没有找到返回 `None` 的情况下再次调用 select_one 引发了这个错误。而如果是以下代码：

```python
ul = soup.select_one('div.swf ul')
if ul:
    print('success')
else:
    print('fail')
```

程序如期打印 `fail` 。

## Python 超时装饰器

* date: 2018-11-17
* brief：

在一个爬虫程序中经常会出现卡死的情况，多次打印日志查看确定卡死的位置在requests 的方法中。

solution：

很容易想到的一个方法就是是否可以控制函数的执行时间，一旦超过了时间便退出重新发起请求。google 发现了func_timeout 这个库。修改 requests 请求的示例代码如下：

```python
import time
from func_timeout import func_timeout, FunctionTimedOut
import requests

url = 'https://www.jobui.com'
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}
try:
    r = func_timeout(100, requests.get, args=(url,), kwargs={'headers': headers})
except FunctionTimedOut:
    print('time out')
except Exception as _e:
    print(_e)
else:
    print(r.status_code)
```

* reference:

https://github.com/kata198/func_timeout

* ghost work:

requests 请求为何会在中间卡死尚未得知，一个猜测可能是https请求的原因。